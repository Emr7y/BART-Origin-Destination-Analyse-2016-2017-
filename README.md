# ğŸš‡ BART Originâ€“Destination Analyse (2016â€“2017)

Dieses Repo enthÃ¤lt eine kompakte, reproduzierbare Analyse der BARTâ€‘Fahrten 2016â€“2017. Wir verknÃ¼pfen die Tripâ€‘Tabelle (Origin/Destination, Throughput, DateTime) mit der Stationstabelle (AbkÃ¼rzung, Name, Koordinaten), normalisieren Codes und beheben historische Aliasâ€‘Abweichungen (z.â€¯B. WSPR â†’ WARM).

Auf Basis der vom Datensatz gelieferten ZÃ¤hlgrÃ¶ÃŸe Throughput (Anzahl pro Zeitintervall, wie bereitgestellt) aggregieren wir:

Topâ€‘Routen (gesamt oder durchschnittlich pro Tag),

Tageszeitâ€‘Muster (Ã˜ Trips pro Stunde),

Wochentagsâ€‘Muster (Ã˜ Trips pro Wochentag).

Der Fokus liegt auf klaren, wenigen Visualisierungen und exportierbaren Tabellen â€“ ideal fÃ¼r eine kurze ErgebnisprÃ¤sentation ohne langatmige EDA im README (die ausfÃ¼hrliche Exploration findet im Notebook statt).

## ğŸ¯ Ziele

* **Topâ€‘Routen** identifizieren (gesamt & Ã˜/Tag)
* **Tageszeitâ€‘Profil** (Stunden) & **Wochentagsâ€‘Profil** (Ã˜/Tag)
* Stationen verknÃ¼pfen (Name + Koordinaten), inkl. **Aliasâ€‘Fix** (z.â€¯B. `WSPR` â†’ `WARM`).

---

## ğŸ“¦ Daten

* **Trips:** `df_1617` mit Spalten: `Origin`, `Destination`, `Throughput`, `DateTime`
* **Stationen:** `df_st` mit Spalten: `Abbreviation`, `Name`, `lat`, `lon`

> Falls Spaltennamen abweichen, bitte unten im â€Setupâ€œ die `rename`â€‘Zeile anpassen.

---

## âš™ï¸ Setup (kurz)

```bash
pip install pandas matplotlib folium  # folium optional
```

---

## ğŸ§­ Minimaler Workflow (Notebook)

### 1) Stationen vorbereiten & Mergen

```python
# Stationstabelle sÃ¤ubern
df_st_fix = (
    df_st.rename(columns={"Abbreviation":"station_code","Name":"station_name"})
         [["station_code","station_name","lat","lon"]]
         .copy()
)
df_st_fix["station_code"] = df_st_fix["station_code"].str.strip().str.upper()

# Trips normalisieren
time_col = "DateTime"
df_1617[time_col] = pd.to_datetime(df_1617[time_col], errors="coerce")
df_1617["date_only"] = df_1617[time_col].dt.date
for c in ["Origin","Destination"]:
    df_1617[c] = df_1617[c].str.strip().str.upper()

# Alias (historische Codes)
alias = {"WSPR":"WARM"}  # Warm Springs/South Fremont
df_1617[["Origin","Destination"]] = df_1617[["Origin","Destination"]].replace(alias)

# Merge: Origin & Destination
DF = (
    df_1617.merge(df_st_fix, how="left", left_on="Origin", right_on="station_code")
            .rename(columns={"lat":"origin_lat","lon":"origin_lon","station_name":"origin_name"})
            .drop(columns=["station_code"])  
)
DF = (
    DF.merge(df_st_fix, how="left", left_on="Destination", right_on="station_code")
      .rename(columns={"lat":"dest_lat","lon":"dest_lon","station_name":"dest_name"})
      .drop(columns=["station_code"])  
)
```

### 2) Topâ€‘15 Routen (gesamt)

```python
keys = [c for c in ("Origin","Destination","origin_name","dest_name") if c in DF.columns]
top15 = (DF.groupby(keys, dropna=False)["Throughput"].sum()
          .reset_index(name="trips").nlargest(15, "trips"))
```

### 3) Stundenprofil & Wochentagsprofil (Ã˜/Tag)

```python
DF["hour"] = DF[time_col].dt.hour
DF["dow"]  = DF[time_col].dt.dayofweek

hourly_avg = (DF.groupby(["date_only","hour"])["Throughput"].sum()
                .groupby("hour").mean())

dow_avg = (DF.groupby(["date_only","dow"])["Throughput"].sum()
             .groupby("dow").mean())
```

### 4) Simple Plots (kompakt)

```python
import matplotlib.pyplot as plt

# Topâ€‘15 Balken (horizontal)
top15["route"] = top15["origin_name"] + " â†’ " + top15["dest_name"]
plt.barh(top15["route"], top15["trips"]); plt.gca().invert_yaxis()
plt.title("Top 15 Routen â€“ Trips gesamt"); plt.xlabel("Trips"); plt.tight_layout(); plt.show()

# Ã˜ Trips pro Stunde
hourly_avg.plot(marker='o', title='Ã˜ Trips pro Stunde'); plt.xlabel('Stunde'); plt.ylabel('Ã˜ Trips'); plt.show()

# Ã˜ Trips pro Wochentag
ax = dow_avg.plot(kind='bar', title='Ã˜ Trips pro Wochentag'); ax.set_xlabel('Wochentag'); ax.set_ylabel('Ã˜ Trips')
ax.set_xticklabels(['Mo','Di','Mi','Do','Fr','Sa','So']); plt.show()
```

---

## ğŸ“¤ Exporte (fÃ¼r Abgabe/GitHub)

```python
DF.to_parquet('bart_trips_enriched.parquet', index=False)
top15.to_csv('bart_top15_total.csv', index=False)
hourly_avg.reset_index().to_csv('bart_hourly_avg.csv', index=False)
dow_avg.reset_index().to_csv('bart_weekday_avg.csv', index=False)
```

---

## ğŸ–¥ï¸ (Optional) Streamlitâ€‘App â€“ Funktionsidee

* Sidebar: **Metrik** wÃ¤hlen (*Trips gesamt* vs. *Ã˜ pro Tag*), **Topâ€‘N** Slider
* Charts: **Topâ€‘Routen** (Barh), **Ã˜/Std** (Line), **Ã˜/Wochentag** (Bar)
* Hinweise: Aliasâ€‘Mapping anzeigen (wenn angewendet)
* Exportâ€‘Buttons: Aggregationen als CSV herunterladen
* Optional: CSVâ€‘Upload eigener Trips

> Wenn gewÃ¼nscht, kann eine kompakte `app.py` (\~60â€“80 Zeilen) generiert werden.

---

## ğŸ“ Repoâ€‘Struktur (Vorschlag)

```
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ bart_analysis.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ trips.csv                 # optional
â”‚   â””â”€â”€ stations.csv              # optional
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ bart_top15_total.csv
â”‚   â”œâ”€â”€ bart_hourly_avg.csv
â”‚   â””â”€â”€ bart_weekday_avg.csv
â”œâ”€â”€ app.py                        # optional (Streamlit)
â””â”€â”€ README.md
```

---

## ğŸ“ Lizenz

MIT (oder nach Bedarf anpassen)

---

**Made with â¤ï¸ by Emr7y**

## ğŸ”— Quellen & Details

* **Modell(e):** Keine MLâ€‘Modelle â€“ deskriptive Analyse (Pandasâ€‘GroupBy/Merge, Matplotlib)
* **Datenquelle:** [BART Ridership (Kaggle)](https://www.kaggle.com/datasets/saulfuh/bart-ridership)
